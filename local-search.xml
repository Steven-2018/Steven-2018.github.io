<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>RNN 循环神经网络</title>
    <link href="undefined2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p><strong>RNN</strong><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image1.png" srcset="/img/loading.gif" alt></p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image2.png" srcset="/img/loading.gif" style="zoom:50%;"><p>Xt 是输入的词向量</p><p>其中Ot 代表t时刻的输出</p><p>St代表t时刻的隐藏层的值</p><p>在整个训练过程中，每一时刻所用的都是同样的W</p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image3.png" srcset="/img/loading.gif" alt></p><p>梯度消失 <strong>vanishing gradient problem</strong></p><p>短期记忆问题是由梯度消失问题引起的==&gt;BP</p><p>Reason:<br>因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0</p><p>这会导致渐变在向后传播时呈指数级收缩。由于梯度极小，内部权重几乎没有调整，因此较早的层无法进行任何学习。这就是消失的梯度问题。</p><p>梯度爆炸</p><p>梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。</p><p>LSTM</p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image4.png" srcset="/img/loading.gif" alt></p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image5.png" srcset="/img/loading.gif" alt></p><p>主线/长期记忆 Ct-1 Ct</p><p>1）遗忘门</p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image6.png" srcset="/img/loading.gif" alt></p><p>Sigmoid，输入h和x，输出0-1值，代表遗忘程度</p><p>2）输入门</p><p>Sigmoid，然后，一个tanh层为新的候选值创建一个向量c，这些值能够加入state中。下一步，我们要将这两个部分合并以创建对state的更新。</p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image7.png" srcset="/img/loading.gif" alt></p><p>3）输出门</p><p>1.首先运行一个sigmoid层，这个也就是输出门，以决定cell<br>state中的那个部分是我们将要输出的</p><p>2.把cell state放进tanh (-1,1)，最后将它与sigmoid门的输出相乘</p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image8.png" srcset="/img/loading.gif" alt></p><p>注意点：</p><ol><li><p>cell 的状态是一个向量，是有多个值的</p></li><li><p>上一次的状态 h(t-1)是怎么和下一次的输入 x(t)<br>结合（concat）起来的，这也是很多资料没有明白讲的地方，也很简单，concat，<br>直白的说就是把二者直接拼起来</p></li><li><p>cell的权重是共享的，这是什么意思呢？这是指这张图片上有三个绿色的大框，代表三个<br>cell 对吧，但是实际上，它只是代表了一个 cell 在不同时序时候的状态</p></li><li><p>cell 最上面的一条线的状态即 s(t) 代表了长时记忆，而下面的 h(t)则代表了工作记忆或短时记忆</p></li></ol><p><strong>BiRNN</strong><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image9.png" srcset="/img/loading.gif" alt></p><p>之前提过的RNN结构都是单向的，但实际问题中还存在不仅依赖于之前的序列还依赖于之后的序列进行预测的问题前向的参数与后向的参数是不共享的</p><p><strong>BiLSTM</strong><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/text.png" srcset="/img/loading.gif" style="zoom: 50%;"></p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image10.png" srcset="/img/loading.gif" alt></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>CNN 卷积神经网络</title>
    <link href="undefined2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p><strong>1.</strong>从标准图中提取出特征<strong>feature</strong></p><p>也被称为卷积核 <strong>filter</strong> 一般3*3 5*5</p><p><img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image1.png" srcset="/img/loading.gif" alt></p><p><strong>2.</strong> 卷积运算</p><p>1）卷积核滑动算乘积</p><p>2）除数量</p><p>得到 feature map （特征图）</p><p>步长 stride=1<img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image7.png" srcset="/img/loading.gif" style="zoom:50%;"></p><p>h[u,v]表示的是卷积核</p><p>F表示的是输入图像</p><p>下面矩阵算的都是坐标</p><p>feature map是每一个feature从原始图像中提取出来的”特征”。其中的值，越接近为1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。</p><p>一个feature作用于图片产生一张feature<br>map，对这张X图来说，我们用的是3个feature，因此最终产生3个 feature map。</p><p><strong>3.</strong> 激活层</p><p>e.g. Relu<img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image2.png" srcset="/img/loading.gif" alt></p><p><strong>4.  pooling</strong>   池化层</p><p>池化分为两种，Max Pooling 最大池化、Average Pooling平均池化。顾名思义，最大池化就是取最大值，平均池化就是取平均值。</p><p><img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image3.png" srcset="/img/loading.gif" alt></p><p>原图片尺寸为9X9，在一系列的卷积、relu、池化操作后，得到尺寸被压缩为2X2的三张特征图。</p><p><strong>常见基础CNN流程</strong></p><p><img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image4.png" srcset="/img/loading.gif" alt></p><p><strong>5.</strong> <strong>全连接层</strong></p><p>对之前的所有操作进行一个总结，给我们一个最终的结果。</p><p>它最大的目的是对特征图进行维度上的改变，来得到每个分类类别对应的概率值。</p><p>卷积层就不是全连接了，卷积层采用的是局部连接”的思想</p><p>该操作，是用一个3X3的图与原图进行连接，很明显原图中只有一个3X3的窗口能够与它连接起来<img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image5.png" srcset="/img/loading.gif" alt></p><p>用的是将窗口滑动起来的方法后续进行连接。这个方法的思想就是参数共享”，参数指的就是filter</p><p>用Softmax（分类函数）算概率</p><p><strong>6. 训练与优化</strong></p><p>===&gt; 训练从W改为卷积核filter</p><p>BP算法:</p><p>监督学习，计算错误或概率相差过大，定义误差：</p><p>梯度下降，反复迭代</p><p>Tips:</p><p><strong>1. Padding</strong></p><p>padding是增加各个边的pixels的数量，目的是保持feature map不要太小，但也没必要超过原图的大小，所以不可以任意数量；</p><p><img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image6.png" srcset="/img/loading.gif" alt></p><p>n 原图片（input）长与宽</p><p>p为padding，f是filter的长与宽， s是stride值</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>NN 神经网络</title>
    <link href="undefined2021/03/03/NN-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>2021/03/03/NN-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p><img src="/2021/03/03/NN-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image1.png" srcset="/img/loading.gif" alt="alt"></p><p>输入层—&gt; 隐藏层 H = X * W1+b1</p><p>隐藏从—&gt; 输出层 Y = H * W2+b2</p><p><img src="/2021/03/03/NN-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image2.png" srcset="/img/loading.gif" alt="alt"></p><p>激活层（激活函数）</p><p>阶跃函数：当输入小于等于0时，输出0；当输入大于0时，输出1。</p><p>Sigmoid：当输入趋近于正无穷/负无穷时，输出无限接近于1/0。</p><p>ReLU：当输入小于0时，输出0；当输入大于0时，输出等于输入。</p><p>Tanh函数：将输入值压缩到 -1到1 的范围，解决了Sigmoid函数的缺陷</p><p><strong>softMax</strong></p><p>通过Softmax层之后，我们得到了I，II，III和IV这四个类别分别对应的概率</p><p>一种直观的解决方法，是用1减去Softmax输出的概率，比如1-90%=0.1。不过更为常用且巧妙的方法是，求对数的负数。概率越接近100%，该计算结果值越接近于0，说明结果越准确，该输出叫做[“]{dir=”rtl”}交叉熵损失（<strong>Cross<br>Entropy Error</strong>）”</p><p>反向传播 <strong>back propagation</strong></p><p>算出交叉熵损失后，就要开始反向传播了</p><p>就是一个参数优化的过程，优化对象就是网络中的所有W和b（因为其他所有参数都是确定的）。</p><p>梯度下降</p><p>然后反复迭代</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>first</title>
    <link href="undefined2019/12/20/foreword/"/>
    <url>2019/12/20/foreword/</url>
    
    <content type="html"><![CDATA[<p>​                                                                        <strong>LOVE AND PEACE</strong></p><p><img src="/2019/12/20/foreword/first.png" srcset="/img/loading.gif" alt="alt"></p>]]></content>
    
    
    <categories>
      
      <category>Personal talk</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="undefined2019/12/18/hello-world/"/>
    <url>2019/12/18/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>