<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>RNN 循环神经网络</title>
    <link href="undefined2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p><strong>RNN</strong><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image1.png" srcset="/img/loading.gif" alt></p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image2.png" srcset="/img/loading.gif" style="zoom:50%;"><p>Xt 是输入的词向量</p><p>其中Ot 代表t时刻的输出</p><p>St代表t时刻的隐藏层的值</p><p>在整个训练过程中，每一时刻所用的都是同样的W</p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image3.png" srcset="/img/loading.gif" alt></p><p>梯度消失 <strong>vanishing gradient problem</strong></p><p>短期记忆问题是由梯度消失问题引起的==&gt;BP</p><p>Reason:<br>因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0</p><p>这会导致渐变在向后传播时呈指数级收缩。由于梯度极小，内部权重几乎没有调整，因此较早的层无法进行任何学习。这就是消失的梯度问题。</p><p>梯度爆炸</p><p>梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。</p><p>LSTM</p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image4.png" srcset="/img/loading.gif" alt></p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image5.png" srcset="/img/loading.gif" alt></p><p>主线/长期记忆 Ct-1 Ct</p><p>1）遗忘门</p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image6.png" srcset="/img/loading.gif" alt></p><p>Sigmoid，输入h和x，输出0-1值，代表遗忘程度</p><p>2）输入门</p><p>Sigmoid，然后，一个tanh层为新的候选值创建一个向量c，这些值能够加入state中。下一步，我们要将这两个部分合并以创建对state的更新。</p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image7.png" srcset="/img/loading.gif" alt></p><p>3）输出门</p><p>1.首先运行一个sigmoid层，这个也就是输出门，以决定cell<br>state中的那个部分是我们将要输出的</p><p>2.把cell state放进tanh (-1,1)，最后将它与sigmoid门的输出相乘</p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image8.png" srcset="/img/loading.gif" alt></p><p>注意点：</p><ol><li><p>cell 的状态是一个向量，是有多个值的</p></li><li><p>上一次的状态 h(t-1)是怎么和下一次的输入 x(t)<br>结合（concat）起来的，这也是很多资料没有明白讲的地方，也很简单，concat，<br>直白的说就是把二者直接拼起来</p></li><li><p>cell的权重是共享的，这是什么意思呢？这是指这张图片上有三个绿色的大框，代表三个<br>cell 对吧，但是实际上，它只是代表了一个 cell 在不同时序时候的状态</p></li><li><p>cell 最上面的一条线的状态即 s(t) 代表了长时记忆，而下面的 h(t)则代表了工作记忆或短时记忆</p></li></ol><p><strong>BiRNN</strong><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image9.png" srcset="/img/loading.gif" alt></p><p>之前提过的RNN结构都是单向的，但实际问题中还存在不仅依赖于之前的序列还依赖于之后的序列进行预测的问题前向的参数与后向的参数是不共享的</p><p><strong>BiLSTM</strong><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/text.png" srcset="/img/loading.gif" style="zoom: 50%;"></p><p><img src="/2021/03/03/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image10.png" srcset="/img/loading.gif" alt></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>CNN 卷积神经网络</title>
    <link href="undefined2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p><strong>1.</strong>从标准图中提取出特征<strong>feature</strong></p><p>也被称为卷积核 <strong>filter</strong> 一般3*3 5*5</p><p><img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image1.png" srcset="/img/loading.gif" alt></p><p><strong>2.</strong> 卷积运算</p><p>1）卷积核滑动算乘积</p><p>2）除数量</p><p>得到 feature map （特征图）</p><p>步长 stride=1<img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image7.png" srcset="/img/loading.gif" style="zoom:50%;"></p><p>h[u,v]表示的是卷积核</p><p>F表示的是输入图像</p><p>下面矩阵算的都是坐标</p><p>feature map是每一个feature从原始图像中提取出来的”特征”。其中的值，越接近为1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。</p><p>一个feature作用于图片产生一张feature<br>map，对这张X图来说，我们用的是3个feature，因此最终产生3个 feature map。</p><p><strong>3.</strong> 激活层</p><p>e.g. Relu<img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image2.png" srcset="/img/loading.gif" alt></p><p><strong>4.  pooling</strong>   池化层</p><p>池化分为两种，Max Pooling 最大池化、Average Pooling平均池化。顾名思义，最大池化就是取最大值，平均池化就是取平均值。</p><p><img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image3.png" srcset="/img/loading.gif" alt></p><p>原图片尺寸为9X9，在一系列的卷积、relu、池化操作后，得到尺寸被压缩为2X2的三张特征图。</p><p><strong>常见基础CNN流程</strong></p><p><img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image4.png" srcset="/img/loading.gif" alt></p><p><strong>5.</strong> <strong>全连接层</strong></p><p>对之前的所有操作进行一个总结，给我们一个最终的结果。</p><p>它最大的目的是对特征图进行维度上的改变，来得到每个分类类别对应的概率值。</p><p>卷积层就不是全连接了，卷积层采用的是局部连接”的思想</p><p>该操作，是用一个3X3的图与原图进行连接，很明显原图中只有一个3X3的窗口能够与它连接起来<img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image5.png" srcset="/img/loading.gif" alt></p><p>用的是将窗口滑动起来的方法后续进行连接。这个方法的思想就是参数共享”，参数指的就是filter</p><p>用Softmax（分类函数）算概率</p><p><strong>6. 训练与优化</strong></p><p>===&gt; 训练从W改为卷积核filter</p><p>BP算法:</p><p>监督学习，计算错误或概率相差过大，定义误差：</p><p>梯度下降，反复迭代</p><p>Tips:</p><p><strong>1. Padding</strong></p><p>padding是增加各个边的pixels的数量，目的是保持feature map不要太小，但也没必要超过原图的大小，所以不可以任意数量；</p><p><img src="/2021/03/03/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image6.png" srcset="/img/loading.gif" alt></p><p>n 原图片（input）长与宽</p><p>p为padding，f是filter的长与宽， s是stride值</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>NN 神经网络</title>
    <link href="undefined2021/03/03/NN-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>2021/03/03/NN-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p><img src="/2021/03/03/NN-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image1.png" srcset="/img/loading.gif" alt="alt"></p><p>输入层—&gt; 隐藏层 H = X * W1+b1</p><p>隐藏从—&gt; 输出层 Y = H * W2+b2</p><p><img src="/2021/03/03/NN-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image2.png" srcset="/img/loading.gif" alt="alt"></p><p>激活层（激活函数）</p><p>阶跃函数：当输入小于等于0时，输出0；当输入大于0时，输出1。</p><p>Sigmoid：当输入趋近于正无穷/负无穷时，输出无限接近于1/0。</p><p>ReLU：当输入小于0时，输出0；当输入大于0时，输出等于输入。</p><p>Tanh函数：将输入值压缩到 -1到1 的范围，解决了Sigmoid函数的缺陷</p><p><strong>softMax</strong></p><p>通过Softmax层之后，我们得到了I，II，III和IV这四个类别分别对应的概率</p><p>一种直观的解决方法，是用1减去Softmax输出的概率，比如1-90%=0.1。不过更为常用且巧妙的方法是，求对数的负数。概率越接近100%，该计算结果值越接近于0，说明结果越准确，该输出叫做[“]{dir=”rtl”}交叉熵损失（<strong>Cross<br>Entropy Error</strong>）”</p><p>反向传播 <strong>back propagation</strong></p><p>算出交叉熵损失后，就要开始反向传播了</p><p>就是一个参数优化的过程，优化对象就是网络中的所有W和b（因为其他所有参数都是确定的）。</p><p>梯度下降</p><p>然后反复迭代</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Goodbye ZJNU</title>
    <link href="undefined2020/06/05/Goodbye-ZJNU/"/>
    <url>2020/06/05/Goodbye-ZJNU/</url>
    
    <content type="html"><![CDATA[<p>说点什么吧，今天毕业了。</p><p>心里空洞洞的不知道说什么，最后一个离开的寝室，有太多的故事发生了，最后几天玩的越开心，离别的时候就越难受</p><p>我本来以为我这个人没什么感情，但还是会难过一下，或许以后大家都再也见不了面了吧。</p><p>把或许去了，是肯定。</p><p><strong>下面是整合了这几年的的总结：</strong></p><p>回想了一下大学这几年，终于不像中学一样把2/3的时光白白挥霍掉，我也不记得到底是什么时候有了留学的想法，反正刚入学绝对没有。刚入学那会我甚至觉得cs根本不需要读硕士而且英语水平也不允许，想着越早步入职场越有利，后来真正实习了我才庆幸这个决定。</p><p>故事大概是大二结束的那个暑假开始的吧，那是我在acm集训队待的最后一个暑假，这一年以来随队也是参加了许多大大小小的比赛，拿的奖不多但以我的智商来说也挺满意的吧，平时也不喜欢钻研后来越来越跟不上跟我小学学奥数一个模样，反正兴趣失去了根本不能成事。那个暑假阿三的缺席让我们队排名倒数第二吧，亚洲赛的资格也没有了，也是时候告别了。离开或许对我来说是个好事，那个每天比赛要排名次，每天都在跟谁谁谁争最后一名，明天赛后都要把排名打在屏幕上鞭尸，每天都在队里丢脸的日子，十几平米的实验室里滋生了我所有的负面情绪</p><p>退队后并没有迷茫，我想着自己努力了两年拿了这些荣誉和绩点不能白费吧，我做了个大胆的决定——考雅思。追溯英语的缺陷好像是从初中开始暴露的，那时一直学不好埋下了恐惧的种子，在高考听力时第一次19.5第二次21彻底让听力成为我的噩梦。四六级听力全蒙（我最后居然能过了四级，至今没考出6级），用了一些雅思的app他们都会打电话问我情况，我不敢说我连6级也没过只好支支吾吾说6级刚过，然后他们会回我一句6级刚过发挥好的话大概也就5.5。说实话我觉得第一次考5.5很不错了，反正半年就一直待图书馆，背单词做剑雅，考前报了两个礼拜不到的新东方大班，还是有点作用的尤其是作文。19年2.27我参加了第一次雅思（total:5.5; 5,6,5.5,5）我觉得5.5还是挺开心，我看到了希望紧接着跟中介签了合同。</p><p>之后的事并没有按照我想的发展，我死都没想到语言考试托了整整一年的时间。接着上次考完我又报了4月底的雅思，但结果告诉我这不到两个月的时间让我没有一点进步。5月一直在找实习，准备了很久的面试材料，线上线下面了数不清的公司，到后来我甚至开始好奇下一家公司会问我啥题，一天最多在杭州面了5家公司，晚上回家9点多了。反正最后找着了，刚开始实习的日子很轻松，我又报名了雅思，仍然是两个月的时间复习。这期间白天上班晚上找各种安静的地方学习回房间继续看到凌晨，965的养老作息能腾出两天时间让我周末去杭州图书馆。那段日子真的很苦头发掉了好多哈哈哈哈，但3个月时间不到，我在专业和英语两个方面都有质变，努力总会有回报的吧。</p><p>7月27号考了第三次，我记得很清楚25号是我的生日，考前我请了半周的假一直泡在图书馆里抱佛脚。考完跟一个高中关系挺好的女孩子一起吃了墨西哥餐hhhhh，出成绩那天中午我坐在hr的车里，准点刷到了成绩（total:6;6,7.5,5.5,5.5）一个很复杂的分数，我终于进步，但仍没到6.5。那天之后我坚定了辞职的想法，隐隐觉得下次一定可以。</p><p>大四上，我又走进了学校的图书馆，闭关，跟考研的同学们一起学习。10.28出成绩我终于结束了,  还记得那天我只是重重的拍了桌子然后跑到天台上跟父母打电话，曾经我幻想过很多庆祝的方式，但还是平淡的结束了一整年的奋斗。后一天我就收到了KCL的拒信，尽管我就申了三所学校但我并不担心，就是迷之自信。最后年前拿到曼大的offer。</p><p>大四下，又去实习了，公司比之前正规多了，虽然活还是不多，跟着leader学了很多东西，虽然最后免不了还是要走的，这次实习最大收获应该是认识了好多杭州人，不管是欢乐的室友还是桌游认识的，还是各种party认识的，玩的很开心唯一的遗憾就是因为实习错过了最后几个月跟大学同学在一起的日子，还有两张爱丁堡的拒信。</p><p>最后，到了毕业前的一周，学校允许回校了，我也请了假，回到了久违的宿舍，第一天就三国杀杀到凌晨，后来的日子也是天天白天OW晚上三国杀，跟着小刘第一次电瓶车逛了全校，作为浙江省占地面积最大的学院不是吹的，去了在图书馆一直望见却从来没去过的红绿别墅，录了视频，我们没有聊理想没有聊现实，有的只有狂欢。我是最后一个离开寝室的，走前对面寝还有阿三没走，说起阿三跟小刘，我们这个team解散完了他们俩就考研去了，结果两个都被调剂了，但他们好像不接受决定二战，我又想起了大二才去高复的严厂，都是命啊。</p><p>我朋友好像一直觉得我是个基佬，从来没见我跟女人在一起过，emmmmmm，那是他们自己宅，难道我还要带到寝室里？？不过也没错，从大一大二竞赛的忙绿到大三的雅思好像总有理由能让自己一直瞎忙，大二时那个女人给的阴影我好像永远放不下，好像我追求过的女生从来都不是好女孩，每次都是。不过后来我自己也变了》。。。反正对外从来都是没谈过恋爱，讲出去也只能博同情吧，宁可假装都没发生过，单身癌晚期。</p><p>写的时候又在听莫吉托，毕业那会这首刚发行，每天都在听，周杰伦果然是我从小听到大的歌手hhhh</p><p>Anyway</p><p>Goodbye ZJNU, Goodbye my friends</p>]]></content>
    
    
    <categories>
      
      <category>Personal talk</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>foreword</title>
    <link href="undefined2019/12/20/foreword/"/>
    <url>2019/12/20/foreword/</url>
    
    <content type="html"><![CDATA[<p>​                                                                        <strong>LOVE AND PEACE</strong></p><p><img src="/2019/12/20/foreword/first.png" srcset="/img/loading.gif" alt="alt"></p>]]></content>
    
    
    <categories>
      
      <category>Personal talk</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="undefined2019/12/18/hello-world/"/>
    <url>2019/12/18/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>